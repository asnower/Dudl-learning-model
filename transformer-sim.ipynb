{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# english前処理\n",
    "#----------------------\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import re\n",
    "import mojimoji\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 文字列処理定義\n",
    "def preprocessing(text):\n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # 数字文字の一律「0」化\n",
    "    # 全角化\n",
    "    return text\n",
    "\n",
    "# Tokenizer定義\n",
    "def one_hot_preprocessing(text):\n",
    "    ones = torch.sparse.torch.eye(5).to(device)\n",
    "    trg=ones.index_select(0,trg).long()\n",
    "# 文字列処理 + Tokenizer\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing(text)\n",
    "    text = text.split(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>', \n",
    "    tokenize=tokenizer_with_preprocessing, \n",
    "    lower=True, \n",
    "    use_vocab=True, \n",
    "    batch_first=True,\n",
    "    fix_length=50\n",
    ")\n",
    "\n",
    "TEXT_act = data.Field(\n",
    "    sequential=False, \n",
    "    tokenize=tokenizer_with_preprocessing, \n",
    "    use_vocab=True, \n",
    "    batch_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙数:20915\n",
      "tensor([ 2, 68, 23,  8,  3,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "       device='cuda:0')\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "#----------------------\n",
    "# データの読込\n",
    "#----------------------\n",
    "train_ds, test_ds = data.TabularDataset.splits(\n",
    "    path='act_emo_text',\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=[('dial', TEXT), ('act', TEXT_act)]\n",
    ")\n",
    "\n",
    "# 確認\n",
    "train_ds[0].__dict__.keys()\n",
    "test_ds[0].__dict__.keys()\n",
    "# for i in range(0, 10):\n",
    "#     print(vars(train_ds[i]))\n",
    "#     print(vars(test_ds[i]))\n",
    "\n",
    "# 辞書作成\n",
    "TEXT.build_vocab(train_ds, test_ds, min_freq=2)\n",
    "TEXT_act.build_vocab(train_ds, test_ds, min_freq=2)\n",
    "# 単語カウント\n",
    "with open('vocab.txt','w') as file:\n",
    "\n",
    "    file.write('\\n'.join(TEXT.vocab.freqs))\n",
    "# print(TEXT.vocab.freqs)\n",
    "print('語彙数:{}'.format(len(TEXT.vocab)))\n",
    "\n",
    "# イテレータの作成\n",
    "# --> 事前に「ランタイムのタイプを変更」からGPUを選択しておく。\n",
    "train_iter = data.Iterator(train_ds, batch_size=16, shuffle=True, device=device)\n",
    "test_iter = data.Iterator(test_ds, batch_size=16, shuffle=False, device=device)\n",
    "\n",
    "# 確認\n",
    "batch = next(iter(train_iter))\n",
    "# print(batch.dial)\n",
    "# print(batch.act)\n",
    "\n",
    "batch = next(iter(test_iter))\n",
    "print(batch.dial[2])\n",
    "print(batch.act.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad_idx, \n",
    "                 trg_pad_idx, \n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "\n",
    "        #src = [batch size, src len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "\n",
    "        #trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        #trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "#          trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        enc_src = enc_src.view(16,-1)\n",
    "        output = self.decoder(enc_src)\n",
    "        return F.softmax(output, dim=1)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "\n",
    "#         output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "#          output, attention = self.decoder(enc_src,)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "#         return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformer_model import Encoder,Decoder\n",
    "#  パラメータの設定\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "OUTPUT_DIM = 5\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "# Encoderの初期化\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "# Decoderの初期化\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "# padding用のIDの指定\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "\n",
    "linear = nn.Linear(HID_DIM*50, OUTPUT_DIM)\n",
    "\n",
    "# モデルの初期化\n",
    "# model = Seq2Seq(enc, dec, PAD_IDX, PAD_IDX, device).to(device)\n",
    "model = Seq2Seq(enc, linear, PAD_IDX, PAD_IDX, device).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7021074701fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "src_mask = self.make_src_mask(src)\n",
    "enc(batch.dial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(16,50).long().to(device)\n",
    "b=torch.randn(16,5).long().to(device)\n",
    "model(batch.dial,batch.act).size()\n",
    "trg = F.one_hot(batch.act,num_classes=5)\n",
    "print(trg.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from train import evaluate,epoch_time,translate_sentence\n",
    "# オプティマイザーの設定\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# 重みの初期化\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "model.apply(initialize_weights)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "\n",
    "        src = batch.dial\n",
    "        trg = F.one_hot(batch.act,num_classes=5)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         output, _ = model(src, trg[:,:-1])\n",
    "        output = model(src, trg)\n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "#         output_size = output.size()\n",
    "#         output_dim = output.shape[-1]\n",
    "\n",
    "#         output = output.contiguous().view(-1, output_dim)\n",
    "#         trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "        \n",
    "#         trg1 = trg.view(16,1,5)\n",
    "#         trg2 = torch.cat(([trg1]*output_size[1]),dim=1)\n",
    "#         assert (output.tolist() >= 0. & output.tolist() <= 1.).all()\n",
    "#         assert (trg.tolist() >= 0. & trg.tolist() <= 1.).all()\n",
    "\n",
    "        loss = criterion(output,torch.max(trg, 1)[1])\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['i', 'guess', 'you', 'are', 'right.but', 'what', 'shall', 'we', 'do', '?', 'i', \"don't\", 'feel', 'like', 'sitting', 'at', 'home', '.']\n",
      "trg = 2\n"
     ]
    }
   ],
   "source": [
    "#----------------------\n",
    "# モデルの学習\n",
    "#----------------------\n",
    "import time\n",
    "import math\n",
    "\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "# サンプル1作品を取得\n",
    "example_idx = 8\n",
    "src_sample = vars(train_ds.examples[example_idx])['dial']\n",
    "trg_sample = vars(train_ds.examples[example_idx])['act']\n",
    "\n",
    "# タイトルと本文を表示\n",
    "print(f'src = {src_sample}')\n",
    "print(f'trg = {trg_sample}')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, test_iter, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    #if valid_loss < best_valid_loss:\n",
    "    #    best_valid_loss = valid_loss\n",
    "    #    torch.save(model.state_dict(), 'drive/My Drive/trained_model.pt')\n",
    "    # 参考スクリプトではバリデーションデータの精度を指標にしているが、今回の試行では学習が進むほどバリデーションデータの精度が低下したため、オーバーフィットは無視して最終エポック後のモデルを採用（途中終了も考慮して各エポック後に保存）\n",
    "    torch.save(model.state_dict(), './Transformer_classify.pt')\n",
    "\n",
    "    # エポックごとに学習用・バリデーション用データの精度を表示\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "    # 10エポックごとにサンプル1作品のタイトルから本文を生成した結果を表示\n",
    "#     if epoch % 10 == 0:\n",
    "#       translation, attention =  m(src_sample, TEXT, TEXT, model, device)\n",
    "#       print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.34789544343948364, 1.4711216688156128, 0.7256169319152832], [-0.1784655898809433, 0.4272070527076721, -0.24276989698410034]]\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn(2,3)\n",
    "list=x.tolist()\n",
    "print(list)\n",
    "for i in enumerate(list):\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(,10)\n",
    "        self.linear2=nn.Linear(10,5)\n",
    "    def forward(self, src):\n",
    "        enc_src = self.linear1(src)\n",
    "        \n",
    "        output = self.linear2(enc_src)\n",
    "        return F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model().to(device)\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# trc=batch.act\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "model.train()\n",
    "epoch_loss = 0\n",
    "for i, batch in enumerate(train_iter):\n",
    "    trg=F.one_hot(batch.act)\n",
    "    src=batch.dial\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src)\n",
    "    loss = criterion(output, trg)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
